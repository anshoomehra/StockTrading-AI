{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Financial Instruments\n",
    "\n",
    "Momentum\n",
    "\n",
    "Alpha\n",
    "\n",
    "Inferential Stats:\n",
    "Null Hypothesis : https://blog.minitab.com/blog/understanding-statistics/things-statisticians-say-failure-to-reject-the-null-hypothesis\n",
    "\n",
    "Signal\n",
    "Noise\n",
    "Signal/Noise Ratio\n",
    "\n",
    "Stock Arbitrage \n",
    "\n",
    "Normality (Normal Distribution) : It is important that our samples & results demostrate normal distribution \n",
    "\n",
    "Probablity Distribution (It is probability that RANDOM SAMPLE set demostrate [tyeOfDistribution] distrution occurs with [xyz] probability \n",
    "> Probablity Distrubution Functions (PDFs) or Type of Distrubutions\n",
    "<img src=\"Images/1.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "\n",
    "What if the Distrbutions are not Standard Normal, & has deviations, how can they can be adjusted to Standard Normal? See the Quiz below, mu is mean & sigma is standard deviatiom \n",
    "<img src=\"Images/2.png\" width=\"500\" height=\"500\" aligh=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terms to know:\n",
    "\n",
    "1. Quartiles\n",
    "2. Deciles\n",
    "3. Quantiles\n",
    "\n",
    "<img src=\"Images/6.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/7.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/8.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Rest below will be covered during content below \n",
    "4. IQR (Inter-quartile Range)\n",
    "5. Whiskers\n",
    "6. Outliers\n",
    "7. Long Tails\n",
    "8. Fat Tails\n",
    "9. Skewness / Kurtosis\n",
    "\n",
    "\n",
    "\n",
    "How do we know that data can be described Normally?? To understand this, there are various tests which can be done as kistd below ...\n",
    "\n",
    "\n",
    "\n",
    "**Comparing Tests for Normality**\n",
    "\n",
    "There are some visual ways to check if a distribution is normally distributed or not. Recall that normal distributions are symmetric and do not have fat tails (a more formal term for “fat tails” is kurtosis”). Box-whisker plots helps us visually check if a distribution is symmetric or skewed. A histogram lets us check if a distribution is symmetric/skewed, and if it has fat tails. \n",
    "\n",
    "<img src=\"Images/3.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "For Normal Distribution:\n",
    "1. Mean is same as Median\n",
    "2. First Quartile and 3rd Quartile are at same distance\n",
    "3. Whiskers are at same distance\n",
    "\n",
    "<img src=\"Images/4.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So how would a Box Plot and Histogram look for data which is not Normal. Below is example of Left Long Fat Tail. Characteristics\n",
    "1. Long tail on one side of the distribution\n",
    "2. Mean lies left to the Median (It is also called as Distrbution is Left Skewed)\n",
    "** In fact stock return tend to exibit left skewed & fat tail, which means, extreme negative returns could occur with greater frequency \n",
    "\n",
    "<img src=\"Images/5.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "To have more thorough test of Normality, we can perform QQ Plot. QQ plots help us compare any two distributions, so they can be used to compare distributions other than the normal distribution. If you plot the actual data’s distribution against a theoretical normal distribution, you can decide if the distributions are the same type if the QQ plot produces a fairly straight line.\n",
    "\n",
    "<img src=\"Images/9.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Normal Distribution QQ Plot - as it draws straight line\n",
    "<img src=\"Images/10.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Not Normal Distribution QQ Plot - as it draws curve line\n",
    "<img src=\"Images/11.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "What if we want single number to test Normality?? If so, we can define threshold which can ease our process of finding Normality i.e. we can define a thereshold like in example below of 5% saying anything above that is Normal and vice-versa, there are few tests which can help achive the goal as listed below.  \n",
    "\n",
    "<img src=\"Images/12.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "There are three hypothesis tests that can be used to decide if a data distribution is normal. These are,\n",
    "1. The Shapiro-Wilk test\n",
    "2. D’Agostino-Pearson, and\n",
    "3. The Kolmogorov-Smirnov test. \n",
    "\n",
    "Each of these produce p-value, and if the p-value is small enough, say 0.05 or less, we can say with a 95% confidence that the data is not normally distributed. Shapiro-Wilk tends to perform better in a broader set of cases compared to the D’Agostino-Pearson test. In part, this is because the D’Agostino-Pearson test is used to look for skewness and kurtosis that do not match a normal distribution, so there are some odd non-normal distributions for which it doesn’t detect non-normality, where the Sharpiro-Wilk would give the correct answer.\n",
    "\n",
    "The Kolmogorov Smirnov test can be used to compare distributions other than the normal distribution, so it’s similar to the QQ plot in its generality. To do a normality test, we would first rescale the data distribution (subtract the mean and divide by its standard deviation), then compare the rescaled data distribution with the standard normal distribution (which has a mean of zero and standard deviation of 1). In general, the Shapiro-WIlk test tends to be a better test than the Kolmogorov Smirnov test, but not in all cases.\n",
    "\n",
    "<img src=\"Images/13.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So in summary, if you want to be thorough, you can use all three tests (there are even more tests that we haven’t discussed here). If you only want to use one test, use the Shapiro-Wilk test. For a sanity check, visualize your data distribution with a histogram, box-whisker plot, and/or a QQ plot.\n",
    "\n",
    "References <br>\n",
    "Shapiro-Wilk test : https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test <br>\n",
    "D’Agostino-Pearson : https://en.wikipedia.org/wiki/D%27Agostino%27s_K-squared_test <br>\n",
    "\n",
    "Kolmogorov-Smirnov : https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stationary Data**: Another thing we must ensure if that data we have is stationary overtime, which means that mean, variance, covariance are same overtime. This can be tested via **Breusch-Pagan Test** which tests 2 properties, Homoscedastic or Heteroskedastic, Homoscedastic mean the data is stationary & Heteroskedastic means it is not-stationary.\n",
    "<br><br>\n",
    "**Homoscedastic vs Heteroskedastic <br>**\n",
    "\n",
    "One of the assumptions of linear regression is that its input data are homoscedastic. A visual way to check if the our data is homoscedastic is a scatter plot \n",
    "<br>\n",
    "If our data is heteroscedastic, a linear regression estimate of the coefficients may be less accurate (further from the actual value), and we may get a smaller p-value than should be expected, which means we may assume (incorrectly) that we have an accurate estimate of the regression coefficient, and assume that it’s statistically significant when it’s not.\n",
    "\n",
    "<img src=\"Images/14.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/15.png\" width=\"500\" height=\"500\" aligh=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> What if our data is Not Normal & Heteroskedastic -- How do we fix this?? </b>\n",
    "\n",
    "We can look or rate of change and apply log on it, one very popular technique to achieve this is Box-Cox Transformation\n",
    "\n",
    "<img src=\"Images/16.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/17.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/18.png\" width=\"500\" height=\"500\" aligh=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression, Multiple Regression, Multi-variate Regression Quick Refresher:** <br>\n",
    "When we build model where one independent variable is used to predict one dependent variable we called it regression, and since we assume that data is lineary seperable, we draw a stright line drawing prediction for dependent variable and hence called Linear Regression. Example: House Price as dependent variable, relying on House Square Footage as independent variable with equation for line as y= mx + c (m is coifficient & c is intercerpt and these can be represented in many different ways like beta, alpha etc) <br>\n",
    "\n",
    "<img src=\"Images/20.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Models goal is to draw the optimal line which makes accurate predictions, however, in reality how much ever optimal line is drawn it can't make 100% accurate predictions, i.e. some error is expected, the error is drawn below as distance from actual to the predicted point (drawn as vertical distances) & are also known as Residuals(error term). Hence, our goal is changed to draw a line with least Residucals.\n",
    "\n",
    "<img src=\"Images/21.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/22.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "We can check if our Residuals follow Normal Distribution i.e. mean of zero and a constant statndard deviation, then these Residuals can be considered Random, by Random we mean that prediction made by model is equally likely be higher or lower than the actual value. If however, te average of the residuals is not zero, this gives us hint that the model has bias in prediction errors.\n",
    "\n",
    "<img src=\"Images/23.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/24.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "**Multiple Regerssion:** Continuing from above, if we see Bias, one way to improve this situation would be to introduce other inpendent variables, and this is called Multiple Regerssion, i.e. we are using more than one independent variable to predict a dependent variable.\n",
    "\n",
    "<img src=\"Images/25.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "**R-Squared** Now, our Gol is to see the model build is performing well, and generalizing predictions, one way this can be done is to do R^2 check on models performance, the value of this test range from 0 to 1. Value of 1 means that all the variations in Dependent Variable can be explained with all the variations in Independent Varaibles. <br>\n",
    "\n",
    "The better metrics is **Adjusted R-Squared** whicj tells us minimum number of independent variables which are most relevant to our model.\n",
    "\n",
    "**F-Test** - Another test which can be performed is called F-Test, whcih tests if Coefficients & Intercept are non-zero & are meaninful, if we get p value < 0.05 that means our model parameters are non-zero and exhibit meaningful relationship.\n",
    "\n",
    "<img src=\"Images/26.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "\n",
    "We studied how multiple independent variables can be used to predict one dependent variable called as Multiple Regression (example, house area, location, # of rooms to predict house price). Now, if we use independent variables to predict multiple dependent variables at the same time is called **Multi-Variate Regression** (example, predicting house price, along with gas & electrcity consumption\n",
    "\n",
    "<img src=\"Images/27.png\" width=\"500\" height=\"500\" aligh=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Breusch-Pagan Test for Heteroscedasticity (revisited)** <br>\n",
    "<br>\n",
    "Now that we’ve covered regression, let’s look at the Breusch Pagan test in more depth. The Breusch-Pagan test is one of many tests for homoscedasticity/heteroscedasticity. It takes the residuals from a regression, and checks if they are dependent upon the independent variables that we fed into the regression. (Note that we’ll explain residuals in a few videos within this lesson, so feel free to jump back here after you watch the video “Linear Regression”). The test does this by performing a second regression of the residuals against the independent variables, and checking if the coefficients from that second regression are statistically significant (non-zero). If the coefficients of this second regression are significant, then the residuals depend upon the independent variables. If the residuals depend upon the independent variables, then it means that the variance of the data depends on the independent variables. In other words, the data is likely heteroscedastic. So if the p-value of the Breusch-Pagan test is ≤ 0.05, we can assume with a 95% confidence that the distribution is heteroscedastic (not homoscedastic).\n",
    "<br>\n",
    "<br>\n",
    "**Breusch-Pagan Test in Python** <br>\n",
    "<br>\n",
    "In Python, we can use the statsmodels.stats.diagnostic.het_breuschpagan(resid, exog_het) function to test for heteroscedasticity. We input the residuals from the regression of the dependent variable against the independent variables. We also input the independent variables that may affect the variance of the data. The function outputs a p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/19.png\" width=\"500\" height=\"500\" aligh=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terms:\n",
    "\n",
    "Non-Stationary Data : Means that the mean and the variance change over time.\n",
    "( \n",
    "\n",
    "What this means for us? So if the data is non-stationary, that means the properties of data change over time, could prove to be conter-productive, as our goal with time series is to use past-data predicting future, and if data is non-stationary, it means that the past data is not very useful to predict future.\n",
    "\n",
    "To counter this in the world of Stocks, \n",
    "1. we use Stock Returns and not prices, since Returns may be more stationary than prices,\n",
    "2. and instead of instead of using returns, we use Log Returns to inhibit properties which are more stationary & evenly distributed  \n",
    "\n",
    ")\n",
    "\n",
    "**AR Models ( Auto Regressive Models )**\n",
    "\n",
    "Auto Regressive models are analogically similar to \"regression\" i.e. they are also follow similar properties, draw a linear seperation i.e. a straight line to draw any predictions based on independent variables. The difference comes in when these models look at history of same independent variable, i.e. seeing data of same variable over various time steps & hence the name autoregression, i.e. regression on self.\n",
    "\n",
    "<img src=\"Images/28.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/29.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "AR Models are defined by Lag, which deteremines how mant previous time steps are used to determine prediction & hence models are named that way, example AR-1 means only one time step prior was used to determine prediction.\n",
    "<img src=\"Images/30.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "To generalize, we refer this as AR(p), i.e. during experimentation stage, various lags can be tested to see which lag value performs the best...\n",
    "<img src=\"Images/31.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "As with any regresison models, Adjusted R-squared test can be used to see models performance.\n",
    "\n",
    "<img src=\"Images/32.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "What if we have multiple time series, if we have multiple time series, since AR model is specific to time series, multiple AR models can be build for each time-series.\n",
    "\n",
    "<img src=\"Images/33.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "However, what if the these multiple time-series have impact on each other i.e. movement of one stock having impact on other, similar to multivariate-multiple regression, we can also have multi-variate version of AR models which are called Vector AR Models.\n",
    "\n",
    "<img src=\"Images/34.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "\n",
    "**MA Models ( Moving Average Models )**\n",
    "In contrast to AR where models relies on prior value of the stock price, Moving Average as name indicates relies on Moving Averages lineraly combined with Residual's or Error Terms (in essence past predictions and actuals delta) . Since the model relies on residulas, which in technical terms has no prior relationship, this model can help uncover sudden changes in the data called white noise.\n",
    "\n",
    "<img src=\"Images/35.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "MA models are also called as MA(q) where q is lag (smilar to AR models) - to find the best value of q can be determined by plotting AutoCorrelation plot, what this tells us is relation between 2 given data points, in this example, we are studying lags and how each value of lag impact prediction, autocorelation is not regression, as regression, multiple factors are combined to collectively see impact on dependent varaible where as corelation measures pair-wise relation between two periods at a time ...\n",
    "\n",
    "<img src=\"Images/36.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So how to read the correlation plot? We would want to use the highly positive and highly negative correlation plot -- (rember highly positively shows strong correlation with positive slope (upward line) and highly negative show strong correlation with negative slope (downward line), so both +1 and -1 strong corelation indicators, values close to zero +ve or -ve means no or little correlation), once we hit values with very little corerlation we should ideally use lag to ignore those values.\n",
    "\n",
    "<img src=\"Images/37.png\" width=\"500\" height=\"500\" aligh=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARMA (Auto Regressive Moving Averages) \n",
    "\n",
    "AR & MA models both capture different characteristics -\n",
    "<br>AR(p) models try to explain the momentum and mean reversion effects often observed in trading markets (market participant effects).\n",
    "<br>\n",
    "MA(q) models try to capture the shock effects observed in the white noise terms. These shock effects could be thought of as unexpected events affecting the observation process e.g. Surprise earnings, wars, attacks, etc.\n",
    "\n",
    "<img src=\"Images/38.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Nice is that, we can get both of these characteristics in model by combining them like below, where p is lag for AR and q is lag for MA:\n",
    "\n",
    "<img src=\"Images/39.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARMA (Auto Regressive Intergrated Moving Averages) \n",
    "\n",
    "Variation of ARMA is ARIMA (Auto Regressive Intergrated Moving Averages). This concept is used in trading strategy called Paris Trading.\n",
    "\n",
    "Taking difference between each period is called 'Time Difference' or 'Item-wise difference' - If we take time-difference of data, we may be able to describe data more easily as a constant-number. If we see the example below, first graph is position of tutle, second is the speed:\n",
    "\n",
    "If we take the derivative of straight-line (first graph), it gives us the slope of the line, slope is single number which can help describe the line with a constant. If we want to go back to position from speed, we can do intergral, this is finding the area under the curve or finding cumulative sum which is nothing but position of turtle in this case.\n",
    "\n",
    "<img src=\"Images/40.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Let's use the fundamentals and apply these to financial data / stocks data.\n",
    "\n",
    "One of the fundamental requirement of ARMA is that data is stationary, if data is non-stationary, which makes it hard to use past to predict future. One way to translate this data into Stationary is to look at time-difference between data, in other words rate of change of the data over certian period, which can be current price divided by previous price. To convert this in to logs, it will be current log price minus previous log price.\n",
    "\n",
    "<img src=\"Images/41.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/42.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "In Assets world, it is considered that their time difference is stationary i.e. their Returns are more stationary than Prices. \n",
    "\n",
    "In math terms, we say that it's\n",
    "    Prices are Intergated of Order 1\n",
    "    And it's Log Returns are Integated of Order 0\n",
    "    \n",
    "<img src=\"Images/43.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So now, we are look at the data and perform \"Augmented Dicky Fuller Test' to see if data is stationary or not. If data is not stationary, we can then perform time-difference & perform the test again as show below:\n",
    "\n",
    "<img src=\"Images/44.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "However, if we data is again non-statinary after first time-difference, another time difference can be applied until we hit stationary data, this is denoted by D value, where D tells, how many times the time-difference is computed. And say that original data is Intergated of Order D (in below example 3) before it turned stationary.\n",
    "\n",
    "<img src=\"Images/45.png\" width=\"500\" height=\"500\" aligh=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal Adjustments using ARIMA (SARIMA)\n",
    "Time series data tends to have seasonal patterns. For instance, natural gas prices may increase during winter months, when it’s used for heating homes. Similarly, it may also increase during peak summer months, when natural gas generators are used to produce the extra electricity that is used for air conditioning. Retail sales also has expected increases during the holiday shopping season, such as Black Friday in the US (November), and Singles’ Day in China (also in November).\n",
    "\n",
    "Stocks may potentially have seasonal patterns as well. One has to do with writing off losses in order to minimize taxes. Funds and individual investors have unrealized capital gains or losses when the stock price increases or decreases from the price at which they bought the stock. Those capital gains or losses become “realized capital gains” or “realized capital losses” when they sell the stock. At the end of the tax year (which may be December, but not necessarily), an investor may decide to sell their underperforming stocks in order to realize capital losses, which may potentially reduce their taxes. Then, at the start of the next tax year, they may buy back the same stocks in order to maintain their original portfolio. This is sometimes referred to as the “January effect.”\n",
    "\n",
    "Removing seasonal effects can help to make the resulting time series stationary, and therefore more useful when feeding into an autoregressive moving average model.\n",
    "\n",
    "To remove seasonality, we can take the difference between each data point and another data point one year prior. We’ll refer to this as the “seasonal difference”. For instance, if you have monthly data, take the difference between August 2018 and August 2017, and do the same for the rest of your data. It’s common to take the “first difference” either before or after taking the seasonal difference. If we took the “first difference” from the original time series, this would be taking August 2018 and subtracting July 2018. Next, to take the seasonal difference of the first difference, this would mean taking the difference between (August 2018 - July 2018) and (August 2017 - July 2017).\n",
    "\n",
    "You can check if the resulting time series is stationary, and if so, run this stationary series through an autoregressive moving average model.\n",
    "\n",
    "Side Note\n",
    "Kendall Lo, one of the subject matter experts of our course, recommends this book: “Way of the Turtle: The Secret Methods that Turned Ordinary People into Legendary Traders”. The book is about how a successful investor trained his students (his “turtles”) to follow his trend-following trading strategy. The book illustrates the concepts of using trading signals, back-testing, position sizing, and risk management. The story is also summarized in this article Turtle Trading: A Market Legend [ https://www.investopedia.com/articles/trading/08/turtle-trading.asp ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kalman Filter\n",
    "\n",
    "What we have studied thus far is that ARMA/ARIMA requires p & q lags to be defined, and these hyper-parameters has huge impact on the performance of the model. What if the previous instead of n lag is represnted by t-1 state itself, this is where Kalman Filter comes in as all prior state observations are represented in t-1 state as single state. Below is high level view of how does Kalman Filter works, just as side note, studied this during Autonomous Car Nanodegree and was dealt in quite detail there.\n",
    "\n",
    "<img src=\"Images/46.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/47.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "As we have seen above, we have noisy data as real-estate and we also have smooth ideal signal curve, which we know after  the fact if past. Kalman filter learn prediction, and see how off it is, further take that feedback and improve the model as shown below, this is high level inution on how all the past observations are translated to single state.\n",
    "\n",
    "<img src=\"Images/48.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "**Kalman Filters for Pairs Trading**\n",
    "One way Kalman Filters are used in trading is for choosing the hedge ratio in pairs trading. We will get into pairs trading and hedge ratios in lesson 13 of this module, but for now, imagine that there’s a magic number that you can estimate from a model, such as a regression model, based on time series data of two stocks.\n",
    "\n",
    "Every day when you get another data point, you can run another regression and get an updated estimate for this number. So do you take the most recent number every time? Do you take a moving average? If so, how many days will you average together? Will you give each day the same weight when taking the average?\n",
    "\n",
    "All of these kinds of decisions are meant to smooth an estimate of a number that is based on noisy data. The Kalman Filter is designed to provide this estimate based on both past information and new observations. So instead of taking a moving average of this estimate, we can use a Kalman Filter.\n",
    "\n",
    "The Kalman Filter takes the time series of two stocks, and generate its “smoothed” estimate for this magic number at each new time period. Kalman Filters are often used in control systems for vehicles such as cars, planes, rockets, and robots. They’re similar to the application in pairs trading because they take noisy indirect measurements at each new time period in order to estimate state variables (location, direction, speed) of a system .\n",
    "\n",
    "Kalman Filters are not used in this module’s project, but if you want to learn more, please check out the extracurricular content section: \"Machine Learning\": Introduction to Kalman Filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Filter\n",
    "\n",
    "It is a type of Genetic Algorithm, by genetic we mean that we apply natural selection to improve the estimates. Partcile filter can be explained with intution of deploying little helpers to make stock predictions, idea is to monitor and provide incentive to helpers who are making right predictions and penalize helpers who can are making bad predictions. Evetually in this process only good helps stay and bad ones are eliminated, since this process demonstrate natural selection, it is referred as type of genetic algorithm. Little helpers are called particles whose parameters are set randomly.\n",
    "\n",
    "<img src=\"Images/53.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "When more particles show similar predictions or are consentrated, this shows confidence in predictions.\n",
    "\n",
    "<img src=\"Images/50.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "However, when predictions are speard out, or partciles show non-similar predictions this shows lack in confidence in predictions.\n",
    "<img src=\"Images/52.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Particle filters are good with handling variety of data as they do not expect data to be normally distributed.\n",
    "Parctile filters do not assume linear relationship so they can fit non-linear data\n",
    "<img src=\"Images/51.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN \n",
    "\n",
    "Will not cover any notes here, look at other Jupter for in depth on RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volatility\n",
    "\n",
    "Volatility is important measure of risk. But what is risk, risk is uncertianity about future & uncertainity about bad thing happening, like loosing all money invested in stocks. We can depict this as standard deviation from the mean of log return, so it measures the dispursion of the log return from the actual log return...\n",
    "\n",
    "<img src=\"Images/54.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So volatility gives you sense of range of values log return will fall into. Let's say over the years, we have studied log return and say that they more or less depict normal distribution, since we know that for normal distribution 95% of data falls under 2 standard deviation +/-, hence if we know the mean and volatility, we can easily compute range of gain as shown in example below ...\n",
    "\n",
    "<img src=\"Images/55.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So why spend so much time studying this, prime reason is that it gives investors sense of what risk they have and what shoudl they expect wrt log returns or how much can they differe from expected, just broadly listing some uses below .. (alpha below means, what investment to trade vs not trade, like defining startegy/portfolio)\n",
    "\n",
    "<img src=\"Images/56.png\" width=\"500\" height=\"500\" aligh=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we calculate the Volatility ?\n",
    "\n",
    "It is basically nothing but calcualting the standard deviation of the data, in our case stock returns, so let's first calculate the stock returns.\n",
    "\n",
    "<img src=\"Images/57.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "The first datum point will not have prior, it will be NAN, so if you now start with n+1 data, you will have n log returns\n",
    "\n",
    "<img src=\"Images/58.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Formula for Volatility (standard deviation) is \n",
    "\n",
    "<img src=\"Images/59.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Key point here is that since Volatility is defined as standard-deviation, it treats log returns below and above the mean the same way, as they are squared while computing standard deviation.\n",
    "\n",
    "Note below some examples, where we are calculating Volatility for different periods in this case, Days vs Weeks, and as it may be evident that Volatility is higher for Weeks as there is possibility of higher price fluctuations expected on weekly data vs Daily Data ..\n",
    "\n",
    "<img src=\"Images/60.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "However, investors would like to see the Volatility which can be compared across data sets and various market situations and hence ideally Annual frequency is used for Volatility comparison, now if we do not have data for several years progression, we can use whatever frequency data we have and extraplote them, and it is called **Annualized Volatility** \n",
    "\n",
    "<img src=\"Images/61.png\" width=\"500\" height=\"500\" aligh=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Annualized Volatility** -- if we do not have data for Annual progressions to calculate Volatility, we can use whatever frequency data we have and extraplote them, and it is called **Annualized Volatility**\n",
    "\n",
    "<img src=\"Images/62.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So formula to calculate the yearly or monthly progression from daily to yearly or monthly to to yearly is below, note 252 is the number of trading days in an year and 12 offcourse is the number of months. \n",
    "\n",
    "<img src=\"Images/63.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So why Square-root & not just multiply by #'s directly? \n",
    "\n",
    "As a quick refresher Natural Log returns sum over-time..\n",
    "\n",
    "<img src=\"Images/64.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Following the analogy, annual log return is the sum of monthly log-returns.\n",
    "\n",
    "<img src=\"Images/65.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Now, let's say that return of each month is Random vriable, which means that it depicts random phenomina & the likelihood that it can take different values can be represented by Probability distributions.\n",
    "\n",
    "<img src=\"Images/66.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Now, let's few assumptions.\n",
    "1. That the log returns of each month have same distributions\n",
    "2. let's say these returns are independent, which means that log return if one month does not depend on other month.\n",
    "\n",
    "<img src=\"Images/67.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Now, we let's say that Annual Log Return is another Random Variable which equates sum of monthly log-returns. recall what it mean to sum random variables, you can generate the distribution of the sum by repeatedly taking sets of samples from each of the Constituent Distributions & summing them.\n",
    "\n",
    "<img src=\"Images/68.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So, we have setup the situation but let's remember our goal, we want to relate the standard deviation of the distrubutions of Monthly log returns to the standard deviation of the distrubutions of Annual log returns.\n",
    "\n",
    "<img src=\"Images/69.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So now, we are going to use the key fact that whenever you sum independent random variables that variance of the sum equals sum of the variances\n",
    "\n",
    "<img src=\"Images/70.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Getting back to the calculation:\n",
    "we now know that Variance of the Annual Log Returns is equal to the Variance of the SUM of Monthly Log Returns.\n",
    "\n",
    "<img src=\"Images/71.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Since, we assumed that monthly log returns were independent, we can also similarly reflect that Variance of the Annual Log Returns is equal to the SUM of Monthly Log Returns Variances\n",
    "\n",
    "<img src=\"Images/72.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Since variance is the square of standard deviation, we can re-write the equation as below:\n",
    "\n",
    "<img src=\"Images/73.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "But, we also assumed that the standard deviations of the monthly log returns were all the same value Sigma Month, so we wend up with 12 factors of Sigma Month. When we do  the square root, we can see the same equation in terms of standard deviation (instead of variance).\n",
    "So this way, we can annulaize the monthly log returns.\n",
    "\n",
    "<img src=\"Images/74.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Similarly, if you want to annulaize the weekly log returns or daily log returns, it can be done similarly as \n",
    "\n",
    "<img src=\"Images/75.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/76.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Windows\n",
    "\n",
    "So we know how to calculate the volatility for a period but in practice we need to see volatility over broader horizon/periods to compare the trend, as it is evident from below graph that S&P 500 dropped significantly in 2008-2009 period and then it had very dramatic trend since then...\n",
    "\n",
    "<img src=\"Images/77.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So to get broader specturm for volatility, we can adopt rolling widows, which essentially gives us volatility for a period and then be compared or be seen as trend, example below ...\n",
    "\n",
    "<img src=\"Images/78.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/79.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So how long this time window should be, one-week, one-month, one-year or anything else, \n",
    "<img src=\"Images/80.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "   Long window means that the value you compute may not react to market conditions as much\n",
    "   Short window means that the value you compute will have more sensitive reaction to market conditions and may not be that realiable\n",
    "    \n",
    "hence, it all depends on the stratgey and individual needs.\n",
    "\n",
    "<img src=\"Images/81.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/82.png\" width=\"500\" height=\"500\" aligh=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponentially Weighted Moving Average\n",
    "\n",
    "If we look at the specturm of the data, the stock movement as of yesterday is more relevant than the 2 months back data. However, we were thus far looking at moving averages which gives equal weightage to 2 months old data vs the most recent, which is not right. To fix this, we can add weight to the equation which will help the old data relevance decay with time, example below:\n",
    "\n",
    "<img src=\"Images/83.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So how do we compute this <br>\n",
    "\n",
    "Let's start with formula of historical volatility:\n",
    "\n",
    "<img src=\"Images/84.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Let's add t subscript to show todays volatility, and square both sides, remeber squaring both sides gives us nothing but variance...\n",
    "\n",
    "<img src=\"Images/85.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Let's make key assumptions,\n",
    "1. That mean log return is zero, it is ok to do as when you do compute return for short interval, their mean is relatively small as compared to their standard deviation.\n",
    "\n",
    "    <img src=\"Images/86.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "2. Now let's change n-1 to n, this influence does not impact much when n is large.\n",
    "\n",
    "    <img src=\"Images/87.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "    \n",
    "So if we see the final equation, it is nothing but the average of the squared log returns in which each of the log returns are weighted equally.\n",
    "\n",
    "   <img src=\"Images/88.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "   \n",
    "So now to make it weighted avergae, let's add a new factor called lambda, lambda is value between 0 & 1, and in this case it is expected to lower down gradually to mellow the impact of the past return and highlight the impact of recent returns\n",
    "\n",
    "   <img src=\"Images/89.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "   \n",
    "<br>\n",
    "So now let's put it all together, we will start with \\( \\lambda^0 \\) which is nothing but 1, and gradually keep increasing it to \\( \\lambda^1 \\), \\( \\lambda^2 \\), \\( \\lambda^3 \\) and so on .. & to make it weighted, divide by the sum of all lambdas\n",
    "\n",
    "   <img src=\"Images/90.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Well this equation threw me off to start with, as it appears that the lambda is increasing with time in past and working in reverse but this is **really veru very interesting equation, as you may see if the starting value of lambda is large it decrease exponentially and if we start is small it increases eponentually** - have done some sample examples below to demostrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          a\n",
      "0  0.806452\n",
      "1  0.161290\n",
      "2  0.032258\n",
      "          a\n",
      "0  0.369004\n",
      "1  0.332103\n",
      "2  0.298893\n",
      "\n",
      "And Just for the fun, to see the impact f big values more prominent & reversing the trend from higher-to-lower to lower-to-higher\n",
      "          a\n",
      "0  0.142857\n",
      "1  0.285714\n",
      "2  0.571429\n"
     ]
    }
   ],
   "source": [
    "n = pd.DataFrame({'a':[.2**0, .2**1, .2**2]})\n",
    "n_sum = .2**0 + .2**1 + .2**2\n",
    "print (n/n_sum)\n",
    "\n",
    "n = pd.DataFrame({'a':[.9**0, .9**1, .9**2]})\n",
    "n_sum = .9**0 + .9**1 + .9**2\n",
    "print (n/n_sum)\n",
    "\n",
    "print(\"\")\n",
    "print (\"And Just for the fun, to see the impact f big values more prominent & reversing the trend from higher-to-lower to lower-to-higher\")\n",
    "\n",
    "n = pd.DataFrame({'a':[2**0, 2**1, 2**2]})\n",
    "n_sum = 2**0 + 2**1 + 2**2\n",
    "print (n/n_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(10))\n",
    "df = pd.DataFrame(np.arange(2, 10, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  2\n",
       "1  4\n",
       "2  6\n",
       "3  8"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0       NaN\n",
       "1  1.000000\n",
       "2  0.500000\n",
       "3  0.333333"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pct_change() #periods=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.140221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.262576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  2.000000\n",
       "1  3.052632\n",
       "2  4.140221\n",
       "3  5.262576"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ewm(alpha=0.1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting Volatility\n",
    "\n",
    "Now we know how to calculate volatility, can we forecast volatility, as that is what most important to us to anticipate the market..\n",
    "\n",
    "- General thinking is that Volatiliy is easier to predict than the Stock Price\n",
    "- Volatility Trend is deemed Sticky, which means one Volatile day is antcipated to be followed by another Volatile Day, some trends below..\n",
    "\n",
    "<img src=\"Images/91.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "To predict Volatility, we can use special form of Auto Regression called **ARCH (Autogeressive Conditionally Hetroscedastic)**\n",
    "Autogeressive means that, present value is somehow related to the recent past values.\n",
    "<br>\n",
    "Hetroscedastic means that variable we are trying to model may have different magnitudes of vriability at different time points. Magnitude of variability is usually measured as variance.\n",
    "<br>\n",
    "Conditional menas that Hetroscedastic property is dependent on previous value or values of this variable.\n",
    "\n",
    "So ARCH formula is very similar to the what we have studied thus far as EMA, the current variance is weight sum of past squared log rerturns, but we think weights as parameters of the models.\n",
    "\n",
    "<img src=\"Images/92.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "But see here, we are only dependent on the log returs, we can also do the same with adding prior variance is dependeny to further generalize it, which also known as **GARCH ( Generalized Autogeressive Conditionally Hetroscedastic ) ** \n",
    "\n",
    "<img src=\"Images/93.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "   \n",
    "these can be parameterized as m,n, where m is # of log return terms and n is number of variance terms..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what causes the Volatility??\n",
    "\n",
    "Most of the people think that some news about the company or asset create price fluctuations, however, study shows that variance in price between days is the leading factor, that is trading itself cause the volatility, higher the volume of trade higher the volatility. So one define various staratigies to be formed on trading volatility, let's look at some observations around these contexts.\n",
    "\n",
    "First one is Mean Reversion\n",
    "<img src=\"Images/94.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Second - In low-volatility times, momentum stratigies may work better.\n",
    "\n",
    "Third - When market is going up Volatility is down, when market is going down Volatility is high. Because of this Volatility is popular media as gauge for fear. To see this in action, let's see VIX index in contrast to S&P ( VIX is nothing but Volatility Index )\n",
    "\n",
    "<img src=\"Images/95.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So can one trade VIX, yes humans will turn anything into tradable entity, VIX is tradable and not just that there is VVIX, which is an index over VIX which can be further traded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to make use of Volatility??\n",
    "\n",
    "some examples below :\n",
    "\n",
    "<img src=\"Images/96.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Let's see what these mean below...\n",
    "\n",
    "**Limiting Universe** means that one can adopt strategy specific to volatility index, low volatility securities show a pattern that they come back to their running mean after the spikes, this can be capatilized, one can buy at spike knowing that prices will hit mean, example below ...\n",
    "\n",
    "<img src=\"Images/97.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Another obseravtion is that low-volatility stocks, outperform high-volatility stocks, but why this happens remain as mystry. One hypothesis could be  that people ignore boring things in light of exciting stocks... And this could be stratgy itself, below are some ETFs which capatilize on low-volatility stocks and have funds spin-off to just focus on them, we will talk about ETF later.\n",
    "\n",
    "<img src=\"Images/98.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "\n",
    "**Normalize by Volatility** means that volatility to be used normalizing. Example say we have momentum signal which says that NFLX and Walmart both has 30% return, this may seems to mean that both are performing same, hwoever, as we know that MFLX can spike 30% return in just a week in contrast to walmart, **so to normalize these spikes, we can divide the returns with Volatility. This is very common practice to compare assets apple-to-apple, specifically when you are comparing them against universe of unrelated stocks.**\n",
    "\n",
    "**Volatility helping with Position Sizes** In general Quants utilize smaller position sizes in their strartegies when markets are volatile, to minimize the volatility of net profits!! commonly called P&L, ( profit and loss ). Generally, P&L across portfolio is measured to see performance and volatility can come in play to define the sizes.\n",
    "\n",
    "<img src=\"Images/99.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So how do we use it in defining position sizes, one of the formulas which can be used is below this will translate to smaller posoition sizes for more expensive and more volatile assets & this is can auto-adjust the portfolio with market adjustents.\n",
    "\n",
    "<img src=\"Images/100.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "This can vbe further gated with thersholds, i.e. to trigger sell or buy defining how much profit or loss one can take in this investment, these are defined as 'take profit level' and 'stop loss level'\n",
    "\n",
    "<img src=\"Images/101.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "However, during high volatile market situations these thresholds reach much sooner than anticipated, and generally analysts adjust them when markets are higly volatile\n",
    "\n",
    "<img src=\"Images/102.png\" width=\"500\" height=\"500\" aligh=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some other breakout strategies\n",
    "\n",
    "**Bollinger Bands** Continuing idea of theresholds we just discussed above, one way to know sell and buy signals is look at standard deviation, i.e. the rolling window, and look at mean if the stock prices in that rolling window, drawing the standard deviation for these rolling windows. Then draw the two standard deviation above and below the rolling mean, this will give is bracket to operate on i.e. trend on price fluctuations -- these lines are called Bollinger Bands. That is if the price say spike up the upper band, but then start return may generate sell signal, and vice-versa, if the price dip below the lower band but start to go up, though not at the bottom but still low can be signal for purchase.\n",
    "\n",
    "**This strategy may work well for stocks whose prices fluctuate but return to the running mean **\n",
    "\n",
    "<img src=\"Images/103.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "However, for stocks where trend is not that variable in nature and it keeps going up, but never hits the upper Bollinger Band as show below\n",
    "\n",
    "<img src=\"Images/104.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "For this trend, the upper & lower band instead of 2 standard deviations can be instead used as Max & Min of Rolling Windows\n",
    "\n",
    "<img src=\"Images/105.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "and one strategy can be formed for this pattern is to define Long Position when price tops rolling max and Short position when price bottoms rolling min.\n",
    "\n",
    "<img src=\"Images/106.png\" width=\"500\" height=\"500\" aligh=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairs Trading and Mean Reversion\n",
    "\n",
    "Before we go in to Paris Trading, some fundamentals on which Paris Trading rely are Mean Reversion and Corelation.\n",
    "\n",
    "**Mean Reversion**, We alerady studied that mean reversion stands for stock price pattern where stock price come back to it's mean post fluctuations. However, during high volatility segments or situations like recession, there may be new mean which may define the new state.\n",
    "\n",
    "<img src=\"Images/106.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Analysts generally do not depend on one stock as such to study patterens, they attempt to pool **corelated stocks** and study the trends in tandum to have more holistic understanding if market, by corelated I mean stocks whose nature of business, valuation, price fluctuations & county of operation has strong relationship with stock being compared with. So given this if one stock reflect pattern which is dramatically different than other, we can assume that this deviation is perhaps temporary and has high proability to revert back to it's mean in future. \n",
    "\n",
    "<img src=\"Images/107.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "To define Mean Reversion mathematically, we will define a model called **Drift & Volatility** Model. In simple words we can say that price of the stock is nothing but sum of it's long term average (Drift) + some randomness (Volatility)\n",
    "\n",
    "**Stock Price = Drift + Volatility**\n",
    "\n",
    "<img src=\"Images/108.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "\n",
    "So mathematically, we can represent it as:\n",
    "\n",
    "<img src=\"Images/109.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Details below:\n",
    "\n",
    "<img src=\"Images/110.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/111.png\" width=\"500\" height=\"500\" aligh=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So let's bring it all together **\n",
    "\n",
    "To carry foward what we studied thus far, we saw concepts of mean reversion & also how stocks can be related or seen in pairs, now the hypothesis building on these concepts is that, if we know of pair of stocks which are related say one company sell peas and other carrots and say tat both are related as these prodicts are sold in pairs then we can determine trends or RELATIVE DIFFERENCES between these two trends, and if we see one diverging from mean then these to generate signals for Short or Long positions. See example below:\n",
    "\n",
    "<img src=\"Images/112.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So there are advantages to trade in pairs, as broader news say jobs data may move all stocks down, however that signal may not be very helpful to make trade decisions, hence seeing the relative difference between pairs combined with that news may generate more confident signal to Short or Long. \n",
    "\n",
    "It may be good to know that though our expectation is that stocks converge back to mean 'over a period of time' - this time period could be in days, weeks, months or may never converge & stratgies must play this factor to define stop loss, should something like this happen to cut down losses.\n",
    "\n",
    "So how do we describe relatve differences between the stocks -\n",
    "\n",
    "**Relative difference is defined by Spread and Hedge Ratios** -- basically our goal is to define ratio of these stocks whcih balance out each other.\n",
    "\n",
    "<img src=\"Images/113.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Note that with pairs trading, we analyze the original stock price series, and do not convert them to returns or log returns. We’ll get into the details shortly, but let’s just look at an example. Let’s say stock_Astock \n",
    "A\n",
    "​\t  is $2 per share, and stock_Bstock \n",
    "B\n",
    "​\t  is $3 per share. If we figured out that we can trade these pairs together, we may go long stock_Astock \n",
    "A\n",
    "​\t  and short stock_Bstock \n",
    "B\n",
    "​\t . But how much do we long stock_Astock \n",
    "A\n",
    "​\t  and short stock_Bstock \n",
    "B\n",
    "​\t ? What if we long 3 shares of stock_Astock \n",
    "A\n",
    "​\t  and short 2 shares of stock_Bstock \n",
    "B\n",
    "​\t ? This is nice, because shares_A \\times price_A - shares_B \\times price_Bshares \n",
    "A\n",
    "​\t ×price \n",
    "A\n",
    "​\t −shares \n",
    "B\n",
    "​\t ×price \n",
    "B\n",
    "​\t  gives us 3 \\times \\$2 - 2 \\times \\$33×$2−2×$3, or zero. Doing pairs trading analysis with the stock price series instead of returns lets us decide how many shares of each stock to long or short, since our goal will be to have the same dollar amount in our long position as in our short position.\n",
    "\n",
    "So to compure this, we first find out the Hedge Ratio, which can be computed two ways\n",
    "\n",
    "<img src=\"Images/114.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Once we know the Hedge Ration, we find the Spread as shown below, this may apper very familiar as it is nothing but finding 'Error Term/Residual' of a Linear Regression (refersher Y - Y_hat). So what below equation is trying is to give us is a model of predicting B depending on A, and estimation Spread that is the variance of price of Stock B minus Estimation of Stock Price B.\n",
    "\n",
    "<img src=\"Images/115.png\" width=\"500\" height=\"500\" aligh=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** SO NEXT BIG Q is HOW DO WE FIND PAIRS FOR TRADES**\n",
    "\n",
    "There could be many factors, and different stratifies can be applied here, somple examples -\n",
    "\n",
    "1. Economic Similarities, depending on say -\n",
    "    - Government Policies: We can say hypothetically that steel stocks are influenced by Govt Polocies in US and may have +/-ve impact in accordance.\n",
    "    - Supply Chain: We can say hypothetically that stocks or chips and dependent on iPhone Sales, a drop in iPhone Sales or vice-versa may signal drop in other Stock.\n",
    "    - Similarly other signals which make pairs depend on each other.\n",
    "    \n",
    "2. Time Zones -  \n",
    "Time zones play very important role for international stocks, say that economic news on Stock A in US may always signal rise in Stock B in India, difference between US and India can play huge role defining strategy when to Buy or Sell Stock B, and these can be cascaded, as if such signals prevail, cascading stocks markets like Germay which comes after India can be leveraged to execute certain stocks in that segment repeaing overall gain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/116.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/118.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/117.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "CoIntegration is not same as Correlation \n",
    "\n",
    "Correlation means when Stock A move up, Stock B moves up, ane vice-versa!\n",
    "\n",
    "CoIntegration means that over a range of days relative increase in A is matched by relative increase in B, lets say we but Stock A worth of $100, and Stocb worth of $100, over a period of time we will see that relative values  remain the same.\n",
    "<img src=\"Images/119.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So what is important for Pairs Trading is CoInegration and the way to find out of Stocks are CoInegrated is done by Engle-Granger Test --\n",
    "<img src=\"Images/120.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "<img src=\"Images/121.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "<img src=\"Images/122.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/123.png\" width=\"500\" height=\"500\" aligh=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we find similar stocks - if we go with analyzing all the stocks it will take lon time, once possible way is to group them by business segments - \n",
    "\n",
    "<img src=\"Images/124.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "But this is very obvious, we would want relationships beyond their business domain, one of the very popular techniques to find similar relationships is by unsupervised Clustering - and this help us find relationships beyond business domains and the ones which hard to study manually.\n",
    "\n",
    "<img src=\"Images/125.png\" width=\"500\" height=\"500\" aligh=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It is also important to define how 2 stocks are diverging from their spread, for this we need to define the threshold, and if we hit the threshold, we will know that it will return back to it's mean ... \n",
    "\n",
    "<img src=\"Images/126.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So when to short or go long ... \n",
    "\n",
    "<img src=\"Images/127.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/128.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/129.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "The way we caculate these threasholds is by checking how many standard viation awat from historical mean ...\n",
    "<img src=\"Images/130.png\" width=\"500\" height=\"500\" aligh=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Testing\n",
    "\n",
    "Very similar to general ML Training, Testing, I will skip explanation...\n",
    "<img src=\"Images/131.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/132.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/133.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/134.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "<img src=\"Images/135.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/136.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/137.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "\n",
    "SUMMARY \n",
    "\n",
    "<img src=\"Images/138.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/139.png\" width=\"500\" height=\"500\" aligh=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/140.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/141.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/142.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "What is an INDEX ?\n",
    "\n",
    "<img src=\"Images/143.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/144.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "More than one Index are called Indicies\n",
    "<img src=\"Images/145.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "<img src=\"Images/146.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/147.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/148.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/149.png\" width=\"500\" height=\"500\" aligh=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/150.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/151.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/152.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/153.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/154.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "<img src=\"Images/155.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/156.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/157.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/158.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/159.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "<img src=\"Images/160.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/161.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "<img src=\"Images/162.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/163.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "<img src=\"Images/164.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "<img src=\"Images/165.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/166.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/167.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/168.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/169.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/170.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "<img src=\"Images/171.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/172.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "<img src=\"Images/173.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/174.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "<img src=\"Images/175.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/175.5.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/176.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/177.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/178.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/179.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/180.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/181.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/182.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/183.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/184.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/185.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/186.png\" width=\"500\" height=\"500\" aligh=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/187.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/188.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/189.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/190.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/191.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "<img src=\"Images/192.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/193.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "<img src=\"Images/194.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/195.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/196.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "<img src=\"Images/197.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/198.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/199.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "<img src=\"Images/200.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/201.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/202.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/203.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/204.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "<img src=\"Images/205.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/206.png\" width=\"500\" height=\"500\" aligh=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio RIsk and Return\n",
    "\n",
    "1. Diversification\n",
    "<img src=\"Images/207.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/208.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Let's see how do diversify the portflio & calculate the gains/strategies based on that ....\n",
    "\n",
    "<img src=\"Images/209.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Weights are nothing but distribution weightage\n",
    "\n",
    "<img src=\"Images/210.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So to calculate portfolio's mean, variance return we will use below formula .. \n",
    "\n",
    "Note that we will need various scenarios (these could be time windows) to come in play to predict the future and that is denoted by 'i'\n",
    "\n",
    "Let's start with PORTFOLIO MEAN: \n",
    "<img src=\"Images/211.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/212.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/213.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/214.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "SO now lets see the PORTFOLIO VARIANCE\n",
    "<img src=\"Images/215.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/216.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/217.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So we know that corelation factor spans between -1 & 1, strongly coorelated stocks would have factor of 1 and the coorelation value far away from 1 signal non-coorelation\n",
    "\n",
    "So let;s assume if stocks are strongly corerlated the replace this factor by 1, what this tells us that portfolio std. deviation is nothing but weighted average of the individual stocks std deviation.\n",
    "\n",
    "<img src=\"Images/218.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "So other corelation factors less than 1 signals the porfilio's std. deviation is less than weighted average of the individual stocks std deviation.\n",
    "\n",
    "So what if corelation factors is -1 \n",
    "\n",
    "In this case portfolio std. deviation is nothing but weighted diffrence of the individual stocks std deviation.\n",
    "\n",
    "<img src=\"Images/219.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "In the case of corelatoon is -1, we get perfectly hedged portfolio by soliving below equation and this signals that variance between 2 stocks is zero, but in reality becuase of market other factors it never reaches 0.\n",
    "\n",
    "<img src=\"Images/220.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "\n",
    "Optional on how these portflio variance is derived \n",
    "\n",
    "<img src=\"Images/221.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "\n",
    "So how does this help us reducing the risk ... \n",
    "\n",
    "\n",
    "<img src=\"Images/222.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "\n",
    "Optional - \n",
    "\n",
    "\n",
    "<img src=\"Images/223.png\" width=\"500\" height=\"500\" aligh=\"left\">\n",
    "<img src=\"Images/224.png\" width=\"500\" height=\"500\" aligh=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far we studied that diversification is key and that portfilio with diversified portflio reduce the risk, now let's see how to compare portfolio's and identify the ones giving is maximum gain with least risk...\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
